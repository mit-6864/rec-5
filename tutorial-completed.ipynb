{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "permanent-librarian",
   "metadata": {},
   "source": [
    "# GPT Tutorial \n",
    "> This code is taken from https://github.com/karpathy/minGPT written by Andrej Karpath. It's under MIT lincense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-serbia",
   "metadata": {},
   "source": [
    "We will implement the GPT model and train it for character-level language modeling objective. We will train it on Shakespare plays.\n",
    "\n",
    "In character-level language modeling, given the previous sequence of characters, we want our model to predict next character.\n",
    "\n",
    "We will use transformer based decoder for this purposes.\n",
    "\n",
    "Then, given the initial promp of a text, we will sample next characters iteratively by using our model. We will show that it can generate coherent text similar to Shakespare plays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-injury",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-robert",
   "metadata": {},
   "source": [
    "GPT is an **autoregressive Transformer decoder** for language modeling.\n",
    "You can find realted papers here:\n",
    "- GPT: https://openai.com/blog/language-unsupervised/\n",
    "- GPT-2: https://openai.com/blog/better-language-models/\n",
    "- GPT-3: https://arxiv.org/abs/2005.14165\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-officer",
   "metadata": {},
   "source": [
    "1) Start by writing `GPT` module. \n",
    "- `GPT` uses an **embedding** layer and a **positional embedding** layer to represent the each token.\n",
    "- Then, it porcesses the initial input with many **Transformer layers** (`Block`)\n",
    "- Each layer is a sequential combination of a 1-hidden-layer MLP block and a **self-attention layer** (`CausalSelfAttention`)\n",
    "- Then, there IS a final **decoder**, just a linear projection.\n",
    "\n",
    "2) Go over the top-k sampler (`sample`) function that we used to sample from the model\n",
    "\n",
    "3) Go over the data loader (`CharDataset`) that reads from the raw text, and gives (x,y) for language modeling objective \n",
    "\n",
    "Today, we will not cover the details of `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "general-tactics",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Parallelizing over: 6 GPUs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "f\"Parallelizing over: {torch.cuda.device_count()} GPUs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-empire",
   "metadata": {},
   "source": [
    "## GPT Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-polymer",
   "metadata": {},
   "source": [
    "We will start by defining a config object which keeps all the configuration values for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "combined-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    \n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for (k,v) in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-portugal",
   "metadata": {},
   "source": [
    "Lets define all the layers and the forward function of GPT module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "allied-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer layers\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        ###\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size() # Batch x Seq_length integers\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "        # forward the GPT model, produce the scores\n",
    "        token_embeddings = self.tok_emb(idx)\n",
    "        position_embeddings = self.pos_emb[:,:t,:]\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-parking",
   "metadata": {},
   "source": [
    "## Transformer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neural-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-thumbnail",
   "metadata": {},
   "source": [
    "## Self Attention with Causal Masking\n",
    "$$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$$\n",
    "We will implement Key-Query-Value attention for this module. We want it to be causal so that each token only attends to its predecesors.   \n",
    "\n",
    "\n",
    "Given a block size input \"hello\", we will set $x = [h,e,l,l]$, and $y=[e,l,l,o]$. With causall attention, we automatically ask model 4 next character predictions at once.\n",
    "  - given just \"h\", please predict \"e\" as next\n",
    "  - given \"he\" please predict \"l\" next\n",
    "  - given \"hel\" predict \"l\" next\n",
    "  - given \"hell\" predict \"o\" next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preceding-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        #Because our attention projection size will be n_embd / n_head  \n",
    "        #effective projection dimension for attention = 768 / 12 \n",
    "        assert config.n_embd % config.n_head == 0  \n",
    "        self.n_head = config.n_head\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1,config.block_size,config.block_size))\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x).view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
    "        v = self.value(x).view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
    "        q = self.query(x).view(B,T,self.n_head,C // self.n_head).transpose(1,2)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        scores = (q @ k.transpose(-2,-1)).div(math.sqrt(q.size(-1)))\n",
    "        scores.masked_fill_(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        y = scores @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-labor",
   "metadata": {},
   "source": [
    "## TopK Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crude-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, _ = torch.topk(logits, k)\n",
    "    return logits.masked_fill(logits < v[:, [-1]], -float('Inf'))\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed    \n",
    "        logits, _ = model(x_cond)     \n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            ix = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-maldives",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "actual-gravity",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this cell really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                return test_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-nothing",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opponent-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        If the block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \"\"\"\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long) # hell\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)  # ello\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "convenient-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bibliographic-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-19 14:58:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2021-03-19 14:58:20 (28.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "applicable-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pleased-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-george",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-valve",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=6*128, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "australian-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wired-perception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God!--O nurse, how shall this be prevented?\n",
      "My husband is on earth, my faith in heaven;\n",
      "How shall that faith return again to earth,\n",
      "Unless that husband send it me from heaven\n",
      "By leaving earth? comfort me, counsel me.\n",
      "Alack, alack, that heaven should practise stratagems\n",
      "Upon so soft a subject as myself!\n",
      "What say'st thou? hast thou not a word of joy?\n",
      "Some comfort, nurse.\n",
      "\n",
      "Nurse:\n",
      "Faith, here it is.\n",
      "Romeo is banish'd; and all the world to nothing,\n",
      "That he dares ne'er come back to challenge you;\n",
      "Or, if he do, it needs must be by stealth.\n",
      "Then, since the case so stands as now it doth,\n",
      "I think it best you married with the county.\n",
      "O, he's a lovely gentleman!\n",
      "Romeo's a dishclout to him: an eagle, madam,\n",
      "Hath not so green, so quick, so fair an eye\n",
      "As Paris hath. Beshrew my very heart,\n",
      "I think you are happy in this second match,\n",
      "For it excels your first: or if it did not,\n",
      "Your first is dead; or 'twere as good he were,\n",
      "As living here and you no use of him.\n",
      "\n",
      "JULIET:\n",
      "Speakest thou from thy heart?\n",
      "\n",
      "Nurse:\n",
      "And from my soul too;\n",
      "Or else beshrew them both.\n",
      "\n",
      "JULIET:\n",
      "Amen!\n",
      "\n",
      "Nurse:\n",
      "What?\n",
      "\n",
      "JULIET:\n",
      "Well, thou hast comforted me marvellous much.\n",
      "Go in: and tell my lady I am gone,\n",
      "Having displeased my father, to Laurence' cell,\n",
      "To make confession and to be absolved.\n",
      "\n",
      "Nurse:\n",
      "Marry, I will; and this is wisely done.\n",
      "\n",
      "JULIET:\n",
      "Ancient damnation! O most wicked fiend!\n",
      "Is it more sin to wish me thus forsworn,\n",
      "Or to dispraise my lord with that same tongue\n",
      "Which she hath praised him with above compare\n",
      "So many thousand times? Go, counsellor;\n",
      "Thou and my bosom henceforth shall be twain.\n",
      "I'll to the friar, to know his remedy:\n",
      "If all else fail, myself have power to die.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "On Thursday, sir? the time is very short.\n",
      "\n",
      "PARIS:\n",
      "My father Capulet will have it so;\n",
      "And I am nothing slow to slack his haste.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "You say you do not know the lady's mind:\n",
      "Uneven is the course, I like it not.\n",
      "\n",
      "PARIS:\n",
      "Immoderately she weeps for Tybalt's death,\n",
      "And therefore have I little talk'd of love;\n",
      "For Venus s\n"
     ]
    }
   ],
   "source": [
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-fundamental",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
